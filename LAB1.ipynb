{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import csv\n",
    "\n",
    "QUALITY_VALUES = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"Z\",\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"z\",\"!\",\"$\",\"%\",\"&\",\"(\",\")\",\"^\",\"#\"]\n",
    "BASES          = [\"A\",\"T\",\"C\",\"G\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1: Random fasta and fastq file generator\n",
    "Write a python program that generates both fasta and fastq files containing reads with the following characteristics:\n",
    "\n",
    "- Read id contains a progressive number starting from 0. \n",
    "- Sequences have length 50 bp\n",
    "- Bases are randomly generated using a A,T,C,G alphabet, but probability of each base for each read should be given from the command line as a set of numbers (probA, probT, probC, probG)\n",
    "- The number of reads should be passed as an argument from the command line\n",
    "- The name of the fasta/fastq file should be passed as an argument from the command line \n",
    "- For fastq files only: the quality of each base is randomly selected.\n",
    "\n",
    "Example:\n",
    "python read_generator simulatedfasta.fa 100 30 30 30 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "args = [\"simulatedfasta.fa\", 100, 30, 30, 30, 10]\n",
    "\n",
    "BASE_PAIRS     = 50\n",
    " \n",
    "OUTPUT_FILE                  =  args[0] \n",
    "READS_NUMBER                 =  args[1]\n",
    "(probA, probT, probC, probG) = (args[2], args[3], args[4], args[5])\n",
    "\n",
    "IS_FASTA = is_fasta(OUTPUT_FILE)\n",
    "IS_FASTQ = is_fastq(OUTPUT_FILE)\n",
    "\n",
    "ARE_PARAMETERS_CORRUPTED = IS_FASTA == IS_FASTQ or (probA + probT + probC + probG != 100) or READS_NUMBER <= 0\n",
    "\n",
    "if ARE_PARAMETERS_CORRUPTED:\n",
    "    sys.exit('There was an error on parameters')\n",
    "\n",
    "probA = normalize_probability(probA)\n",
    "probT = normalize_probability(probT)\n",
    "probC = normalize_probability(probC)\n",
    "probG = normalize_probability(probG)\n",
    "\n",
    "probability_intervals = define_probability_intervals(probA, probT, probC, probG)\n",
    "\n",
    "reads          = list()\n",
    "quality_values = list()\n",
    "\n",
    "for i in range(READS_NUMBER):\n",
    "    read    = generate_read(BASE_PAIRS, probability_intervals)\n",
    "    quality = generate_quality_value(BASE_PAIRS) if IS_FASTQ == True else \"\"\n",
    "    \n",
    "    reads         .append(read)\n",
    "    quality_values.append(quality)\n",
    "    \n",
    "make_file(OUTPUT_FILE, reads, quality_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2: Statistics extraction\n",
    "\n",
    "Write a python program for extracting statistics from fasta/fastq files. The program must take as a first argument from the command line the name of the input fasta file to be analyzed and write to an output text file (whose name is passed as a second argument from the command line) a summary of the computed statistics.\n",
    "The following are the expected output statistics:\n",
    "- Statistics of single bases across all the reads: Number of A,T,C,G\n",
    "- Number of reads having at least one low complexity sequence: AAAAAA, TTTTTT, CCCCCC or GGGGGG.\n",
    "- Number of reads having the number of GC couples (so called GC content) higher than a threshold GC_THRESHOLD passed as third argument from the command line\n",
    "- For each read having a GC content higher than GC_THRESHOLD, report the read_id and the number of GC couples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [\"simulatedfasta.fa\", \"analysis_simulatedfasta.txt\", 3]\n",
    "\n",
    "LOW_COMPLEXITY_THRESHOLD = 6\n",
    "LOW_COMPLEXITY_SEQUENCES = [base * LOW_COMPLEXITY_THRESHOLD for base in BASES]\n",
    "\n",
    "INPUT_FILE   = args[0]\n",
    "OUTPUT_FILE  = args[1]\n",
    "GC_THRESHOLD = args[2]\n",
    "\n",
    "IS_FASTA = is_fasta(INPUT_FILE)\n",
    "IS_FASTQ = is_fastq(INPUT_FILE)\n",
    "\n",
    "ARE_PARAMETERS_CORRUPTED = IS_FASTA == IS_FASTQ \n",
    "\n",
    "if ARE_PARAMETERS_CORRUPTED:\n",
    "    sys.exit('There was an error on parameters')\n",
    "\n",
    "reads = read_file(INPUT_FILE)  \n",
    "\n",
    "single_bases_occurrences       = get_occurrences_of_single_bases             (reads)\n",
    "low_complexity_sequences_reads = get_number_of_low_complexity_sequences_reads(reads)\n",
    "gc_content_reads               = get_number_of_gc_contents_reads             (reads)\n",
    "\n",
    "make_analysis_file(OUTPUT_FILE, single_bases_occurrences, low_complexity_sequences_reads, gc_content_reads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3: Fasta comparison\n",
    "\n",
    "Write a python program to compare two fasta files. The two fasta files are passed as first and second argument from the command line.\n",
    "\n",
    "The two fasta files have the following characteristics:\n",
    "- The fasta format of the two files is correct (no need to check the format) \n",
    "- Each read can take up one or multiple lines\n",
    "- Each input file does not contain duplicated reads (i.e. identical reads)\n",
    "\n",
    "The program must write as output a third fasta file containing only the reads that are in common between the input files. \n",
    "The read ids in the output file should be composed by the read id of the first file concatenated with the read id of the second file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [\"simulatedfasta1.fa\", \"simulatedfasta2.fa\"]\n",
    "\n",
    "INPUT_FILE_1 = args[0]\n",
    "INPUT_FILE_2 = args[1]\n",
    "OUTPUT_FILE  = \"common_reads_\" + INPUT_FILE_1.replace(\".fa\", \"\") + \"_\" + INPUT_FILE_2.replace(\".fa\", \"\") + \".txt\"\n",
    "\n",
    "reads_1 = read_fasta_file(INPUT_FILE_1)\n",
    "reads_2 = read_fasta_file(INPUT_FILE_2)\n",
    "\n",
    "common_reads = list(set(reads_1).intersection(reads_2))\n",
    "\n",
    "make_common_reads_file(OUTPUT_FILE, common_reads, reads_1, reads_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 4: Consensus region\n",
    "\n",
    "Write a python program that reconstructs the consensus regions on a specific chromosome starting from a tab-separated file called alignments.txt made up of three columns:<br> the read ID, the sequence of the read and the alignment position of the read onto the reference genome. <br> An example of alignments.txt is available in the following. <br><br>\n",
    "Exploiting the sequence and the alignment position of each read, build the consensus regions on the selected chromosome. <br>\n",
    "Please note that all reads have the same length and that multiple consensus regions are allowed for the same chromosome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ATTTCGGCGGCGACACACCGATGACACTAAGCACG', 'GCATTTAAAAAATCCTTGGACACAAATGCAT']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = [\"alignment.txt\", \"reference_genome.txt\"]\n",
    "\n",
    "ALIGNMENT_FILE        = args[0]\n",
    "REFERENCE_GENOME_FILE = args[1]\n",
    "\n",
    "mock_alignment_file(ALIGNMENT_FILE)\n",
    "\n",
    "reference_genome = \"\".join(read_file(REFERENCE_GENOME_FILE))\n",
    "\n",
    "reads = read_alignment_file(ALIGNMENT_FILE)\n",
    "\n",
    "regions = [Base(i) for i in range(len(reference_genome))]\n",
    "regions = fill_regions(reads, regions)\n",
    "regions = [region.current_base for region in regions]\n",
    "\n",
    "produce_consensus_regions(regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_fasta(filename):\n",
    "    return True if filename.find(\".fa\") != -1 else False\n",
    "\n",
    "def is_fastq(filename):\n",
    "    return True if filename.find(\".fq\") != -1 else False\n",
    "\n",
    "def read_file(filename):\n",
    "    global IS_FASTA, IS_FASTQ\n",
    "    \n",
    "    if IS_FASTA == True:\n",
    "        return read_fasta_file(filename)\n",
    "\n",
    "    if IS_FASTQ == True:\n",
    "        return read_fastq_file(filename)\n",
    "\n",
    "def read_fasta_file(filename, multiline = False):\n",
    "    if multiline == True:\n",
    "        return read_fasta_file_multiline(filename)\n",
    "    \n",
    "    reads = list()\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\">\"):\n",
    "                continue\n",
    "                \n",
    "            reads.append(line.rstrip(\"\\n\"))\n",
    "    \n",
    "    f.close()\n",
    "    return reads\n",
    "\n",
    "def read_fasta_file_multiline(filename):\n",
    "    sequence = \"\"\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\">\"):\n",
    "                if len(sequence) > 0:\n",
    "                    reads.append(sequence)\n",
    "                    sequence = \"\"\n",
    "                continue\n",
    "                \n",
    "            sequence += sequence.rstrip(\"\\n\")\n",
    "    \n",
    "    f.close()\n",
    "    return reads\n",
    "    \n",
    "def read_fastq_file(filename):\n",
    "    reads = list()\n",
    "    skip  = False\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\">\"):\n",
    "                continue\n",
    "            \n",
    "            if skip == False:\n",
    "                reads.append(line.rstrip(\"\\n\"))\n",
    "            \n",
    "            skip = not skip\n",
    "    \n",
    "    f.close()\n",
    "    return reads\n",
    "\n",
    "def read_alignment_file(ALIGNMENT_FILE):\n",
    "    f              = open(ALIGNMENT_FILE, \"r\") \n",
    "    alignment_file = csv.reader(f, delimiter=\"\\t\")\n",
    "    \n",
    "    reads = list()\n",
    "    \n",
    "    for row in alignment_file:\n",
    "        reads.append( (row[1], int(row[2])) )\n",
    "\n",
    "    return reads\n",
    "\n",
    "def normalize_probability(prob):\n",
    "    return prob/100\n",
    "\n",
    "def define_probability_intervals(probA, probT, probC, probG):\n",
    "    probability_intervals = dict()\n",
    "    \n",
    "    cumulative_probability     = probA\n",
    "    probability_intervals[\"A\"] = (0,                          cumulative_probability - 0.01)\n",
    "    \n",
    "    old_cumulative_probability = cumulative_probability\n",
    "    cumulative_probability    += probC\n",
    "    probability_intervals[\"C\"] = (old_cumulative_probability, cumulative_probability - 0.01) \n",
    "    \n",
    "    old_cumulative_probability = cumulative_probability\n",
    "    cumulative_probability    += probT\n",
    "    probability_intervals[\"T\"] = (old_cumulative_probability, cumulative_probability - 0.01)\n",
    "    \n",
    "    old_cumulative_probability = cumulative_probability\n",
    "    cumulative_probability     = 1\n",
    "    probability_intervals[\"G\"] = (old_cumulative_probability, cumulative_probability)\n",
    "    \n",
    "    return probability_intervals\n",
    "\n",
    "def generate_read(BASE_PAIRS, probability_intervals):\n",
    "    global BASES\n",
    "    \n",
    "    read = \"\"\n",
    "    \n",
    "    for i in range(BASE_PAIRS):\n",
    "        base  = generate_base(BASES, probability_intervals)\n",
    "        \n",
    "        read += base\n",
    "    \n",
    "    return read\n",
    "\n",
    "def generate_quality_value(BASE_PAIRS):\n",
    "    global QUALITY_VALUES\n",
    "    \n",
    "    quality_value = \"\"\n",
    "    \n",
    "    for i in range(BASE_PAIRS):\n",
    "        quality_value_index = random.randint(0, len(QUALITY_VALUES) - 1)\n",
    "        \n",
    "        quality_value      += QUALITY_VALUES[quality_value_index]\n",
    "\n",
    "    return quality_value\n",
    "\n",
    "def generate_base(BASES, probability_intervals):\n",
    "    random_value = round(random.random(), 2)\n",
    "    \n",
    "    for base in BASES:\n",
    "        low, high = probability_intervals[base]\n",
    "        if low <= random_value <= high:\n",
    "            return str(base)\n",
    "    \n",
    "    return str(base)\n",
    "\n",
    "def get_occurrences_of_single_bases(reads):\n",
    "    sequences = \"\".join(reads)\n",
    "    \n",
    "    return {\n",
    "        \"A\": sequences.count(\"A\"),\n",
    "        \"T\": sequences.count(\"T\"),\n",
    "        \"C\": sequences.count(\"C\"),\n",
    "        \"G\": sequences.count(\"G\")\n",
    "    }\n",
    "\n",
    "def get_number_of_low_complexity_sequences_reads(reads):\n",
    "    global LOW_COMPLEXITY_SEQUENCES\n",
    "    \n",
    "    matching_reads = 0\n",
    "    \n",
    "    for read in reads:\n",
    "        if any(low_complexity_sequence in read for low_complexity_sequence in LOW_COMPLEXITY_SEQUENCES):\n",
    "            matching_reads += 1\n",
    "            \n",
    "    return matching_reads\n",
    "    \n",
    "def get_number_of_gc_contents_reads(reads):\n",
    "    global GC_THRESHOLD\n",
    "    \n",
    "    matching_reads = list()\n",
    "    \n",
    "    for i in range(len(reads)):\n",
    "        read = reads[i]\n",
    "        gc_contents = read.count(\"GC\")\n",
    "        \n",
    "        if gc_contents >= GC_THRESHOLD:\n",
    "            matching_reads.append( (i, gc_contents) )\n",
    "            \n",
    "    return matching_reads\n",
    "\n",
    "def produce_consensus_regions(regions):\n",
    "    \n",
    "    region           = \"\"\n",
    "    consensus_region = list()\n",
    "    \n",
    "    for base in regions:\n",
    "        \n",
    "        if base != \"\":\n",
    "        \n",
    "            region += base\n",
    "\n",
    "        elif base == \"\" and region != \"\":\n",
    "            \n",
    "            consensus_region.append(region)\n",
    "            region = \"\"\n",
    "    \n",
    "    consensus_region.append(region)\n",
    "    \n",
    "    return consensus_region\n",
    "        \n",
    "\n",
    "def fill_regions(reads, regions):\n",
    "    \n",
    "    for sequence, index in reads:\n",
    "        \n",
    "        for base in list(sequence):\n",
    "            \n",
    "            regions[index].fill(base)\n",
    "            \n",
    "            index += 1\n",
    "    \n",
    "    return regions\n",
    "\n",
    "def make_file(filename, reads, quality_values):\n",
    "    global IS_FASTA, IS_FASTQ\n",
    "    \n",
    "    f = open(filename, \"w\")\n",
    "    \n",
    "    for i in range(len(reads)):\n",
    "        f.write(\">read\" + str(i) + \"\\n\")\n",
    "        f.write(reads[i] + \"\\n\")\n",
    "        \n",
    "        if IS_FASTQ == True:\n",
    "            f.write(quality_values[i] + \"\\n\")\n",
    "\n",
    "    f.close()\n",
    "\n",
    "def make_analysis_file(filename, single_bases_occurrences, low_complexity_sequences_reads, gc_content_reads):\n",
    "    global BASES, GC_THRESHOLD\n",
    "    \n",
    "    with open(filename, \"w\") as f:\n",
    "        for base in BASES:\n",
    "            f.write(\"Number of occurrences of base \" + base + \": \" + str(single_bases_occurrences[base]) + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"Number of reads having at least one low complexity sequence: \" + str(low_complexity_sequences_reads) + \"\\n\")\n",
    "        f.write(\"Number of reads having the number of GC content higher than \" + str(GC_THRESHOLD) + \": \" + str(len(gc_content_reads)) + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        for read in gc_content_reads:\n",
    "            f.write(\"read\" + str(read[0]) + \",\\t\" + str(read[1]) + \"\\n\")\n",
    "    \n",
    "    return\n",
    "       \n",
    "def make_common_reads_file(filename, common_reads, reads_1, reads_2):\n",
    "    if any(common_reads):\n",
    "        with open(filename, \"w\") as f:\n",
    "            for read in common_reads:\n",
    "                f.write(\">read\" + str(reads_1.index(read)) + \"_\" + str(reads_2.index(read)) + \"\\n\" + read)\n",
    "                \n",
    "def mock_alignment_file(filename):\n",
    "    content = [\n",
    "        (\"CAGCCATGACACTAAGCACG\", 15),\n",
    "        (\"TTTAAAAAATCCGTGGACAC\", 40),\n",
    "        (\"GCATTTAAAAAATCCTTGGA\", 37),\n",
    "        (\"ATTTCGGCGGCGACACCCCG\", 0 ),\n",
    "        (\"TTCGGCGGCGACACACCGAT\", 2 ),\n",
    "        (\"ATATTTGGACACAAATGCAT\", 48)\n",
    "    ]\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        for i in range(len(content)):\n",
    "            f.write(\"read_\" +  str(i) + \"\\t\" + str(content[i][0]) + \"\\t\" + str(content[i][1]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base:\n",
    "    def __init__(self, index):\n",
    "        self.index        = index\n",
    "        self.current_base = \"\"\n",
    "        self.counters     = {\n",
    "            \"A\" : 0,\n",
    "            \"C\" : 0,\n",
    "            \"T\" : 0,\n",
    "            \"G\" : 0\n",
    "        }\n",
    "        pass\n",
    "    \n",
    "    def fill(self, base_found):\n",
    "\n",
    "        self.counters[base_found] += 1\n",
    "        \n",
    "        if self.current_base == \"\" or (self.current_base != base_found and self.counters[base_found] > self.counters[self.current_base]):\n",
    "            self.current_base = base_found\n",
    "\n",
    "        return\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
